# =====================================================
# FLEXIMART ETL PIPELINE
# RAW → TRANSFORM → LOAD (WITH ERROR HANDLING & LOGGING)
# =====================================================

import pandas as pd
import numpy as np
import re
import csv
from datetime import datetime
from sqlalchemy import create_engine, text
from sqlalchemy.exc import SQLAlchemyError

# =====================================================
# CONFIG
# =====================================================
import os
import csv
from datetime import datetime

# =====================================================
# PATH CONFIG (PORTABLE & REPO-RELATIVE)
# =====================================================

# Project root (one level above this script)
PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))

# Input data directory
DATA_PATH = os.path.join(PROJECT_ROOT, "data")

# Output directory (same folder as this script)
OUTPUT_PATH = os.path.dirname(__file__)

# Log file path
LOG_FILE = os.path.join(OUTPUT_PATH, "etl_log.csv")


# =====================================================
# LOGGING SETUP
# =====================================================

def log_event(stage, level, message):
    with open(LOG_FILE, mode="a", newline="", encoding="utf-8") as f:
        writer = csv.writer(f)
        writer.writerow([datetime.now(), stage, level, message])

# Create log file header if not exists
if not os.path.exists(LOG_FILE):
    with open(LOG_FILE, mode="w", newline="", encoding="utf-8") as f:
        writer = csv.writer(f)
        writer.writerow(["timestamp", "stage", "level", "message"])

log_event("STARTUP", "INFO", "ETL script started")

# =====================================================
# DATABASE CONNECTION
# =====================================================

MYSQL_USER = "root"
MYSQL_PASSWORD = "gkk123GKK"
MYSQL_HOST = "localhost"
MYSQL_PORT = "3306"
MYSQL_DB = "fleximart"
engine = create_engine(
    f"mysql+mysqlconnector://{MYSQL_USER}:{MYSQL_PASSWORD}@{MYSQL_HOST}:{MYSQL_PORT}/{MYSQL_DB}"
)


# =====================================================
# METRICS (FOR TERMINAL OUTPUT)
# =====================================================
metrics = {
    "customers": {"processed": 0, "duplicates_removed": 0, "missing_handled": 0},
    "products":  {"processed": 0, "duplicates_removed": 0, "missing_handled": 0},
    "sales":     {"processed": 0, "duplicates_removed": 0, "missing_handled": 0},
    "loaded":    {"customers": 0, "products": 0, "orders": 0, "order_items": 0}
}

# =====================================================
# EXTRACT
# =====================================================
try:
    customers_df = pd.read_csv(os.path.join(DATA_PATH, "customers_raw.csv"))
    products_df  = pd.read_csv(os.path.join(DATA_PATH, "products_raw.csv"))
    sales_df     = pd.read_csv(os.path.join(DATA_PATH, "sales_raw.csv"))

    metrics["customers"]["processed"] = len(customers_df)
    metrics["products"]["processed"]  = len(products_df)
    metrics["sales"]["processed"]     = len(sales_df)

    log_event("EXTRACT", "INFO", "Raw CSV files loaded successfully")

except Exception as e:
    log_event("EXTRACT", "ERROR", str(e))
    raise

# =====================================================
# TRANSFORM: CUSTOMERS
# =====================================================

#store customer_id  values to customer_code
#this will help when loading autogenerated customer_id 
customers_df["customer_code"] = customers_df["customer_id"]

#Empty strings should not be treated as missing by pandas
#Treat email as a regualr expression, catch any - "", " ", " \t " etc.
#replace those values with NaN (missing value)
customers_df["email"] = customers_df["email"].replace(r"^\s*$", np.nan, regex=True)
missing_email_count = customers_df["email"].isna().sum()#count of rows that have missing emails

#fill all missing emails with customer_id_unknown@dummy.com
customers_df["email"] = customers_df.apply(
    lambda r: f"{r['customer_code']}_unknown@dummy.com"
    if pd.isna(r["email"]) else r["email"],
    axis=1
)

metrics["customers"]["missing_handled"] = int(missing_email_count) #Replaces missing_handled with number of missing emails found.

before = len(customers_df) #Count how many customer rows you have before removing duplicates

#Remove duplicate customers based on email. Keep the first occurrence.Drop any later rows with the same email
customers_df = customers_df.drop_duplicates(subset=["email"], keep="first")
metrics["customers"]["duplicates_removed"] = before - len(customers_df)#Replaces duplicates_removed with number of duplicates found.

#Stadardize phone number
def standardize_phone(phone):
    if pd.isna(phone):
        return None
    digits = re.sub(r"\D", "", str(phone))
    return "+91-" + digits[-10:] if len(digits) >= 10 else None

customers_df["phone"] = customers_df["phone"].apply(standardize_phone)

#handling date of any format 
customers_df["registration_date"] = pd.to_datetime(
    customers_df["registration_date"],
    errors="coerce",
    format="mixed",
    dayfirst=True
).dt.strftime("%Y-%m-%d")

customers_mysql_df = customers_df.drop(columns=["customer_id", "customer_code"])

# =====================================================
# TRANSFORM: PRODUCTS
# =====================================================
products_df["product_code"] = products_df["product_id"]

before = len(products_df)
products_df = products_df.drop_duplicates(subset=["product_name"], keep="first")
metrics["products"]["duplicates_removed"] = before - len(products_df)

price_missing = products_df["price"].isna().sum() #count of missing price rows
products_df["price"] = products_df["price"].fillna(products_df["price"].mean()) #Replaces only the missing prices with mean
metrics["products"]["missing_handled"] += int(price_missing) #metrics to record how many missing price values were handled.

stock_missing = products_df["stock_quantity"].isna().sum() #count of missing stock quantity rows
products_df = products_df.dropna(subset=["stock_quantity"]) #drop the rows that have no enteries for stock qty
metrics["products"]["missing_handled"] += int(stock_missing) #metrics to record how many missing stock values were handled.

products_df["category"] = (
    products_df["category"].astype(str).str.strip().str.lower().str.capitalize()
) #convert first letter to capital letter for category

#Drop product_id and product_code pairs from product_df.
#This is to auto-generate product_id and handle insertion of prodcut_code(varchar) into INT column
#Prevents primary key conflicts when inserting into MySQL
#Keeps natural keys (product_code) out of the core dimension table
products_mysql_df = products_df.drop(columns=["product_id", "product_code"])

# =====================================================
# LOAD: DIMENSION TABLES (WITH ERROR HANDLING)
# =====================================================
try:
    with engine.begin() as conn:
        conn.execute(text("SET FOREIGN_KEY_CHECKS=0"))
        conn.execute(text("TRUNCATE TABLE order_items"))
        conn.execute(text("TRUNCATE TABLE orders"))
        conn.execute(text("TRUNCATE TABLE customers"))
        conn.execute(text("TRUNCATE TABLE products"))
        conn.execute(text("SET FOREIGN_KEY_CHECKS=1"))

    customers_mysql_df.to_sql("customers", engine, if_exists="append", index=False)
    products_mysql_df.to_sql("products", engine, if_exists="append", index=False)

    metrics["loaded"]["customers"] = len(customers_mysql_df)
    metrics["loaded"]["products"]  = len(products_mysql_df)

    log_event("LOAD_DIMENSIONS", "INFO", "Customers & Products loaded successfully")

except SQLAlchemyError as e:
    log_event("LOAD_DIMENSIONS", "ERROR", str(e))
    raise

# =====================================================
# READ BACK SURROGATE KEYS
# =====================================================
customers_db = pd.read_sql("SELECT customer_id, email FROM customers", engine)
products_db  = pd.read_sql("SELECT product_id, product_name FROM products", engine)

customer_lookup = customers_df[["customer_code", "email"]].merge(
    customers_db, on="email", how="left"
)[["customer_code", "customer_id"]]

product_lookup = products_df[["product_code", "product_name"]].merge(
    products_db, on="product_name", how="left"
)[["product_code", "product_id"]]

# =====================================================
# TRANSFORM: SALES
# =====================================================
before = len(sales_df)
sales_df = sales_df.drop_duplicates()
metrics["sales"]["duplicates_removed"] = before - len(sales_df)

sales_df["transaction_date"] = pd.to_datetime(
    sales_df["transaction_date"],
    errors="coerce",
    format="mixed",
    dayfirst=True
).dt.strftime("%Y-%m-%d")

sales_df["status"] = sales_df["status"].fillna("Pending")#fill empty status with Pending

#Join sales with customer mapping, keep the real customer ID, and remove the old code.
sales_df = sales_df.merge(
    customer_lookup,
    left_on="customer_id",
    right_on="customer_code",
    how="left"
).rename(columns={"customer_id_y": "customer_id"}).drop(
    columns=["customer_id_x", "customer_code"]
)
#Join sales with products mapping, keep the real product ID, and remove the old code.”
sales_df = sales_df.merge(
    product_lookup,
    left_on="product_id",
    right_on="product_code",
    how="left"
).rename(columns={"product_id_y": "product_id"}).drop(
    columns=["product_id_x", "product_code"]
)

missing_refs = sales_df[["customer_id", "product_id"]].isna().any(axis=1).sum() #count of missing cust id and prod id
metrics["sales"]["missing_handled"] = int(missing_refs) #metrics to record how many missing custid or prodid values were handled.

if missing_refs > 0:
    print(f"\n Dropping {missing_refs} sales rows due to missing FK mappings")

sales_df = sales_df.dropna(subset=["customer_id", "product_id"])#drop rows with no data for custid and prodid
sales_df["customer_id"] = sales_df["customer_id"].astype(int)#make sure id is INT
sales_df["product_id"] = sales_df["product_id"].astype(int)#make sure id is INT

sales_df["subtotal"] = sales_df["quantity"] * sales_df["unit_price"] #calculate subtotal

# =====================================================
# LOAD: ORDERS & ORDER_ITEMS (ERROR HANDLED)
# =====================================================
# Group sales rows into one order per transaction
try:
    orders_df = (
        sales_df
        .groupby(
            ["transaction_id", "customer_id", "transaction_date", "status"],
            as_index=False
        )
        .agg(total_amount=("subtotal", "sum"))
    )

    # Assign sequential order_id values
    orders_df.insert(0, "order_id", range(1, len(orders_df) + 1))
    # Rename date column and select required fields
    orders_mysql_df = orders_df.rename(
        columns={"transaction_date": "order_date"}
    )[["order_id", "customer_id", "order_date", "status", "total_amount"]]

    orders_mysql_df.to_sql("orders", engine, if_exists="append", index=False)#Load orders to sql
    metrics["loaded"]["orders"] = len(orders_mysql_df)    # Record number of orders loaded

    # Join sales back to orders using transaction_id
    sales_with_orders = sales_df.merge(
        orders_df[["transaction_id", "order_id"]],
        on="transaction_id",
        how="inner"
    )
    # Each row represents one product in an order
    order_items_mysql_df = sales_with_orders[
        ["order_id", "product_id", "quantity", "unit_price", "subtotal"]
    ]

    order_items_mysql_df.to_sql("order_items", engine, if_exists="append", index=False)#Load orders to sql
    metrics["loaded"]["order_items"] = len(order_items_mysql_df)#Record number of order items loaded

    log_event("LOAD_FACTS", "INFO", "Orders & Order Items loaded successfully")#log load

except SQLAlchemyError as e:
    log_event("LOAD_FACTS", "ERROR", str(e))
    raise

# =====================================================
# FINAL TERMINAL OUTPUT (UNCHANGED)
# =====================================================
print("\n===== ETL SUMMARY =====")
for k, v in metrics.items():
    print(k.upper(), ":", v)

print("\n✅ ETL completed successfully.")

# =====================================================
# DATA QUALITY REPORT (TEXT FILE OUTPUT)
# =====================================================
REPORT_FILE = os.path.join(OUTPUT_PATH, "data_quality_report.txt")

with open(REPORT_FILE, "w", encoding="utf-8") as f:
    f.write("===== ETL SUMMARY =====\n")
    f.write(f"CUSTOMERS : {metrics['customers']}\n")
    f.write(f"PRODUCTS : {metrics['products']}\n")
    f.write(f"SALES : {metrics['sales']}\n")
    f.write(f"LOADED : {metrics['loaded']}\n")

print("\n✅Data quality report saved.")

log_event("REPORT", "INFO", "Data quality report generated")
